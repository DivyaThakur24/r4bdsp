[
["index.html", "R for the Bertelsmann Data Science Scholarship Program Welcome", " R for the Bertelsmann Data Science Scholarship Program TBD Welcome This is the website for “R for the Bertelsmann Data Science scholarship Program”. This book will be an R-based supplemental resource for those participating in the Bertelsmann Data Science Scholarship Program. It will contain sections of the curriculum translated into R and also supplemental tutorials on specific topics using R. "],
["sql-intro.html", "1 Introduction", " 1 Introduction This portion of the book focuses on translating Lessons 28-32 of the Bertelsmann Data Science Scholarship Program. These lessons provide the foundation for working with databases using Structured Query Language (SQL). Like the lessons within the scholarship program, we will using data from the fictional Parch and Posey company. You can access this data by installing the parchposey package from GitHub: devtools::install_github(&quot;jdbarillas/parchposey&quot;) You will learn how to recreate the analyses in R using the dplyr package from the tidyverse in the following ways: dplyr provides a set of verbs that are combined to solve data manipulation problems. "],
["introduction.html", "2 Introduction 2.1 Contributors", " 2 Introduction This part of the book provides tutorials on specific topics using R. These tutorials are written and curated by scholarship participants. 2.1 Contributors 2.1.1 Afton - Map Visualizations My name is Afton and I work as a statistical programmer for Murphy Research, a market research company based in Los Angeles. My background is originally in linguistics, and I graduated with an MA in linguistics from the University of Southern California. Connect with me on Twitter or LinkedIn so we can talk more about data and language! 2.1.2 Jolynn - Udacity Even Lessons My name is Jolynn. I am a spatial data analyst with a masters in GIS from Penn State, where I studied remote sensing and spatial statistics. You can see some of my maps on my website or find me on LinkedIn. 2.1.3 How to contribute We are using a shared repository model. Therefore, you can contribute to the book by following these steps: Clone the book repo from GitHub. Create a separate branch for your topic Add the file(s) to the proper directories and commit the changes to your topic branch Push the changes to GitHub and submit a pull request For information on the shared repository model, see here and here. "],
["topics-lessons.html", "3 Udacity Even Lessons 3.1 Lesson 2 Visualization 3.2 Lesson 4 Variability 3.3 Lesson 6 Normal Distribution", " 3 Udacity Even Lessons 3.1 Lesson 2 Visualization In this exercise we will start by looking at the characteristics of our dataset. All definitions and data come from the original Udacity lesson and can be found here. 3.1.1 Data We start by assigning our data to Petals, a list of petal counts from flowers. Petals &lt;- c(15, 16, 17, 16, 21, 22, 15, 16, 15, 17, 16, 22, 14, 13, 14, 14, 15, 15, 14, 15, 16, 10, 19, 15, 15, 22, 24, 25, 15, 16) 3.1.2 Frequency The frequency of a data set is the number of times a certain outcome occurs. # Find the most frequent petal count. # to do this we create a table from Petals, sort it in decreasing order, and get the name of the first item. MostFreq &lt;- names(sort(table(Petals),decreasing=TRUE)[1]) print(paste0(&quot;The most frequent petal count is: &quot;, MostFreq)) #&gt; [1] &quot;The most frequent petal count is: 15&quot; # Find the frequency of flowers with 15 petals. # We can use the same method as above but without the names PetalFreq &lt;- sort(table(Petals),decreasing=TRUE)[1] print(paste0(&quot;The frequency of flowers with 15 petals is: &quot;, PetalFreq)) #&gt; [1] &quot;The frequency of flowers with 15 petals is: 9&quot; 3.1.3 Proportions A proportion is the fraction of counts over the total sample. # Find the proportion of flowers with 15 petals TotalSample &lt;- length(Petals) PetalProp &lt;- PetalFreq/TotalSample print(paste0(&quot;The proportion of flowers with 15 petals is: &quot;, PetalProp)) #&gt; [1] &quot;The proportion of flowers with 15 petals is: 0.3&quot; 3.1.4 Percentage A proportion can be turned into a percentage by multiplying the proportion by 100. # Find the percentage of flowers with 15 petals PetalPerc &lt;- PetalProp * 100 print(paste0(&quot;The percentage of flowers with 15 petals is: &quot;, PetalPerc, &quot;%&quot;)) #&gt; [1] &quot;The percentage of flowers with 15 petals is: 30%&quot; 3.1.5 Histogram A histogram is a graphical representation of the distribution of data, discrete intervals (bins) are decided upon to form widths for our boxes. R has the hist() function to make basic histograms. Here are some very simple examples. # Create a histogram with a bin size of 2 hist(Petals, breaks = (length(Petals)/2), col = &quot;blue&quot;) # Create a histogram with a bin size of 5 bins &lt;- c(10,15,20,25,30) #hist(Petals, breaks = (length(Petals)/5)) hist(Petals, breaks = bins, col = &quot;orange&quot;) 3.1.6 Skew Positive Skew - A positive skew is when outliers are present along the right most end of the distribution. The histogram for Petals is an example of a positive skew Negative Skew - A negative skew is when outliers are present along the left most end of the distribution 3.2 Lesson 4 Variability In this exercise we will start by looking at the variability of our dataset. All definitions and data come from the original Udacity lesson and can be found here. 3.2.1 Data We start by assigning data to income and looking at the summary statistics. I am also going to grab all of the stats to use for labels. income &lt;- c(2500, 3000, 2900 ,2650, 3225, 2700 ,2740, 3000, 3400 ,2500, 3100, 2700) summary(income) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2500 2688 2820 2868 3025 3400 #names(income_stats) Income_min &lt;- min(income) Income_max &lt;- max(income) Income_med &lt;- median(income) Income_lowQ &lt;- unname(quantile(income, c(0.25))) Income_highQ &lt;- unname(quantile(income, c(0.75))) 3.2.2 Interquartile range The Interquartile range (IQR) is the distance between the 1st quartile and 3rd quartile and gives us the range of the middle 50% of our data. The IQR is easily found by computing: Q3 - Q1 3.2.3 Box Plot A good way to view this is using a box plot. The IQR will be displayed within the box between Q1 and Q3. boxplot(income, col = &quot;lightblue&quot;) text(x = .75,y = Income_min, labels = &quot;min&quot;) text(x = .75,y = Income_max, labels = &quot;max&quot;) text(x = .75,y = Income_med, labels = &quot;median&quot;) text(x = .75,y = Income_lowQ, labels = &quot;Q1&quot;) text(x = .75,y = Income_highQ, labels = &quot;Q3&quot;) 3.2.4 Outliers You can use the IQR to identify outliers: Upper outliers: Q3 + (1.5 * IQR) Lower outliers: Q1 - (1.5 * IQR) IQR &lt;- Income_highQ - Income_lowQ upper_outlier &lt;- Income_highQ + (1.5 * IQR) lower_outlier &lt;- Income_lowQ - (1.5 * IQR) cat(paste0(&quot;The Income IQR = $&quot;,IQR,&quot;\\n&quot;, &quot;Upper Outliers are above: $&quot;, upper_outlier,&quot;\\n&quot;, &quot;Lower Outliers are below: $&quot;,lower_outlier)) #&gt; The Income IQR = $337.5 #&gt; Upper Outliers are above: $3531.25 #&gt; Lower Outliers are below: $2181.25 3.2.5 Variance The variance is the average of the squared differences from the mean. The formula for computing variance is: \\[\\sigma^{2} = \\frac{\\sum_{i=1}^{n} \\left(x_{i} - \\bar{x}\\right)^{2}} {n-1}\\] # first let&#39;s calculate it from scratch # get the mean of income s_mean &lt;- mean(income) # get the difference between each income and the mean r_diff &lt;- function(x){x - s_mean} diff_of_mean &lt;- r_diff(income) # get the sqr_root of each data point sqr_of_diff &lt;- diff_of_mean^2 #sum the squares sum_sqrs &lt;- sum(sqr_of_diff) # Divide by the number of samples - 1 Var_income &lt;- sum_sqrs / (length(income) - 1) print(paste0(&quot;The variance caluclated by hand: &quot;, Var_income)) #&gt; [1] &quot;The variance caluclated by hand: 81033.9015151515&quot; # we can also use the built in function in R print(paste0(&quot;The variance using the R built in function: &quot;,var(income))) #&gt; [1] &quot;The variance using the R built in function: 81033.9015151515&quot; 3.2.6 Standard Deviation The standard deviation is the square root of the variance and is used to measure distance from the mean. In a normal distribution 65% of the data lies within 1 standard deviation from the mean,95% within 2 standard deviations, and 99.7% within 3 standard deviations. std_income &lt;- round(sqrt(Var_income),2) print(paste0(&quot;The standard deviation caluclated by hand: &quot;, std_income)) #&gt; [1] &quot;The standard deviation caluclated by hand: 284.66&quot; # we can also use the built in function in R print(paste0(&quot;The standard deviation using the R built in function: &quot;,round(sd(income),2))) #&gt; [1] &quot;The standard deviation using the R built in function: 284.66&quot; pos_1_std &lt;- s_mean + std_income pos_2_std &lt;- s_mean + std_income * 2 neg_1_std &lt;- s_mean - std_income neg_2_std &lt;- s_mean - std_income * 2 Let’s visualize this. I add in lines for one and two standard deviations but, because the data are not normal the standard deviations fall outside of the ranges defined by the three sigma rule of thumb. hist(income, probability = TRUE, col = &quot;orange&quot;) lines(density(income)) abline(v = s_mean, col = &quot;blue&quot;, ) abline(v = pos_1_std, col=&quot;blue&quot;, lty = &quot;dashed&quot;) abline(v = neg_1_std, col=&quot;blue&quot;, lty = &quot;dashed&quot;) abline(v = pos_2_std, col=&quot;red&quot;, lty = &quot;dashed&quot;) abline(v = neg_2_std, col=&quot;red&quot;, lty = &quot;dashed&quot;) text(s_mean - 20,.0015, expression(mu)) text(pos_1_std - 40,.0015, expression(mu + sigma)) text(neg_1_std - 40,.0015, expression(mu - sigma)) text(pos_2_std - 40,.0015, expression(mu + 2*sigma)) 3.3 Lesson 6 Normal Distribution In this exercise we will find the probability of a given observation within a normal distribution. All definitions and data come from the original Udacity lesson and can be found here. #install.packages(&quot;ggfortify&quot;) #library(ggfortify) 3.3.1 Probability Distribution Function. The probability distribution function is a normal curve with an area of 1 beneath it, to represent the cumulative frequency of values. 3.3.1.1 Data mean=1.85; sd=.15 lb=1.7; ub=2 #generate student heights x &lt;- seq(-4,4,length=100)*sd + mean hx &lt;- dnorm(x,mean,sd) 3.3.1.2 Create a density plot plot(x, hx, type=&quot;n&quot;, xlab=&quot;Student Height Values&quot;, ylab=&quot;Density&quot;, main=&quot;Probability Distribution&quot;) lines(x, hx) polygon(c(lb,x,ub), c(0,hx,0), col=&quot;blue&quot;) 3.3.2 Finding the probability If given an observation, you can find the probability and show the area below, above, and between particular observations. To do this you must first calculated the z-score 3.3.2.1 Z-Score \\[ z=\\frac{x-\\mu}{\\sigma}\\] The z-score is calculated by taking the observation minus the mean and dividing by the standard deviation. If given an observation of 2.05 meters the z-score would be calculated as follows: obs1 &lt;- 2.05 obs1_zscore &lt;- round((obs1 - mean)/sd, 2) print(paste0(&quot;The Z-Score for a student with the Height of 2.05 meters is: &quot;, obs1_zscore)) #&gt; [1] &quot;The Z-Score for a student with the Height of 2.05 meters is: 1.33&quot; 3.3.2.2 Proportion using a z-table From this we know that a height of 2.05 is 1.33 standard deviations away from the mean. With this information we can use a z-table to determine the proportion of students that are shorter than this. The z-table can be found here. From the z-table we get a proportion of .9082 or 90.82 percent. A person with the height of 2.05 is taller than 90.82 percent of the students. We can calculate this in R using the pnorm function. It will give a more exact number than when using the table. 3.3.2.3 Proportion using pnorm #you can also use pnorm to get the proportion rather than looking it up in a z-table obs1_spor &lt;- round(pnorm(obs1, mean, sd),4) print(paste0(&quot;The proportion of students shorter than 2.05 meters is: &quot;, obs1_spor)) #&gt; [1] &quot;The proportion of students shorter than 2.05 meters is: 0.9088&quot; 3.3.2.4 Plotting the percentage By multiplying the proportion by 100 you can get the percentage of students that are shorter than 2.05 meters. You can also find the percentage of students that are taller than 2.05 meters by subtracting the proportion from 1. # a plot of the data plot(x, hx, type=&quot;n&quot;, xlab=&quot;Student Height Values&quot;, ylab=&quot;Density&quot;, main=&quot;Percentage above and below 2.05 meters&quot;) # to plot the proportion I grab the samples that are above and below the observation. i &lt;- x &lt;= obs1 o &lt;- x &gt;= obs1 lines(x, hx) # I use the gathered high and low observations to make polygons showing the area. polygon(c(lb,x[i],obs1), c(0,hx[i],0), col=&quot;blue&quot;) polygon(c(obs1,x[o],ub), c(0,hx[o],0), col=&quot;yellow&quot;) #label the percentage taller_obs1 &lt;- round((1 - obs1_spor) * 100, 2) shorter_obs1 &lt;- obs1_spor * 100 text(1.8,.3, labels = shorter_obs1, col = &quot;white&quot;) text(2.1,.3, labels = taller_obs1, col = &quot;blue&quot;) 3.3.2.5 Proportion of a range You can find the proportion of students that fall between a given range by subtraction the proportion of the first observation from the proportion of the second observation. obs2 &lt;- 1.87 obs2_spor &lt;- round(pnorm(obs2, mean, sd),4) # a plot of the data plot(x, hx, type=&quot;n&quot;, xlab=&quot;Student Height Values&quot;, ylab=&quot;Density&quot;, main=&quot;Percentage between 1.87 and 2.05 meters&quot;) # to plot the proportion I grab the samples that are between the observations. i_diff &lt;- x &gt;= obs2 &amp; x &lt;= obs1 lines(x, hx) # I use the gathered range of observations to make polygons showing the area. polygon(c(obs2,x[i_diff],obs1), c(0,hx[i_diff],0), col=&quot;blue&quot;) #label the percentage between_obs &lt;- round((obs1_spor - obs2_spor) * 100, 2) text(1.95,.3, labels = between_obs, col = &quot;white&quot;) "],
["topics-map-viz.html", "4 Map Visualizations 4.1 Background 4.2 How Maps Work 4.3 Plotting Data Over a Map", " 4 Map Visualizations 4.1 Background This is a quick intro to plotting data over a map in R. For this exercise, we’ll look at an old trending topic on social media – Yanny vs. Laurel. As a quick background story: In May of 2018, a post about a short audio clip which sounded like the word “laurel” to some listeners and “yanny” to others was widely shared (and debated) on social media. The audio clip itself sounds like a poor representation of (English language) pronunciation of both “laurel” and “yanny”, and is rather something in between the two. People sometimes distinctly heard one or the other – either “laurel” or “yanny” but not the other. When linguist Chelsea Sanker commented on the Laurel/Yanny phenomenon, she suggested that your regional dialect might have an influence on which one you percieved, and that more demographic information would be useful in considering this possibility. In this exercise, we will perform an exploratory analysis of Yanny vs. Laurel by geographic region by plotting a dataset of tweets about the topic over a map of the United States. 37,064 unique tweets containing the word “yanny” or the word “laurel” (or both) were gathered via the Twitter API using the twitteR package. These tweets included retweets, since it was assumed that retweeting also signified contributing an opinion to the debate. From these tweets, the Twitter API was used again to retrieve the location information (if any) listed in the bio of each relevent accounts. It is beyond the scope of this little tutorial to go into that part of the analysis, so we from this point forward we will just start with resulting dataset. If you’re interested in using the Twitter API, you should! There is documentation on the Twitter website and tutorials on how to do this in the R language. After gathering the data, a “winner”&quot; was calculated, by tweet, by tallying “laurel” mentions vs. “yanny” mentions and taking the highest tally. This winner formed the metric of a speaker’s choice. This approach assumed that a speaker would mention more often the form that they prefer – for example: “When I say yanny… do you hear laurel?” - Coworker. Bruhhhh what? #laurel (Laurel wins!) This may cause divorce. My husband hears it wrong. #Laurel https://t.co/FTmCsZoY8i (Laurel!) Transervice is team Yanny… what about you Yanny or Laurel, what do you hear? https://t.co/lxG6p8g01I (Yanny wins!) I stg this is causing a war in my family. 3/5 hear #yanny while 2/5 hears #laurel I’m 100% team Yanny https://t.co/PG7twEhLbB (Yanny!) RT @LoriLoughlin: I hear Yanny. #yannyvslaurel #Yanny https://t.co/fUo9jDDqhp (Yanny again!) Under this metric, some speakers were “ties” and so they won’t contribute to the analysis – for example, this one: Brands tryna capitalize on #Yanny and #Laurel https://t.co/gz74Mwb6SX (Nobody wins) If a user tweeted multiple times on the subject, only their first tweet with a “win” was counted. The user location data gathered via the Twitter API was then transformed from its raw form to a a U.S. state location. For example, if a user’s bio location contained either “California” or “CA”,their full state name was coded as “California.” This means non-U.S. regions and ambiguous regions or misspellings were skipped over, and users who chose not to provide a location were also skipped. This brought the sample size down to 6,499 tweets with U.S. state locations. The result was then merged with a set of of states, census regions, and larger regions compiled from information provided by U.S. Census. Plotting by state or census region acts as a rough proxy for dialect. Dialects may not align perfectly with state or region boundaries – e.g. a state or a city could contain many dialects. But since all we have to go on is Twitter bios, we won’t get perfect alignment with dialectal boundaries. This is a rough approximation that can be nice for an exploratory analysis. That’s all the background info, so let’s load the data and get into the map-making! dataset contains the Twitter data with the Cenusus information included, and state contains the Census information by itself. dataset &lt;- read.csv(&quot;data/anonymizedDedupedDataset.csv&quot;, stringsAsFactors = FALSE) states &lt;- read.csv(&quot;data/states.csv&quot;, stringsAsFactors = FALSE) 4.2 How Maps Work When you plot a map using R, you generally need two things: a shape to map over (representing the geographic areas and boundaries) and a dataset including location. Currently, our Twitter dataset has the following variables: #&gt; [1] &quot;text&quot; &quot;favorited&quot; &quot;favoriteCount&quot; &quot;replyToSN&quot; #&gt; [5] &quot;created&quot; &quot;truncated&quot; &quot;replyToSID&quot; &quot;id&quot; #&gt; [9] &quot;replyToUID&quot; &quot;statusSource&quot; &quot;screenName&quot; &quot;retweetCount&quot; #&gt; [13] &quot;isRetweet&quot; &quot;retweeted&quot; &quot;longitude&quot; &quot;latitude&quot; #&gt; [17] &quot;location&quot; &quot;State&quot; &quot;Abbreviation&quot; &quot;CensusRegion&quot; #&gt; [21] &quot;LargerRegion&quot; &quot;laurelCount&quot; &quot;yannyCount&quot; &quot;choice&quot; and the state, state abbreviation, cencuses, and larger region variables look like this: State Abbreviation CensusRegion LargerRegion washington WA Pacific West Plotting a map geom requires the ggplot2 package, and the map we would like to plot over can be found in the fiftystater package. That makes our task a little easier, because fiftystater already contains the lognitude and latitude coordinates for plotting a map. All that remains is to match the Twitter dataset back to fiftystater by means of the identifying variable, U.S. state name. As an example, let’s plot a map right now just using fiftystater. It won’t have the Twitter data in it yet, but that’s alright. Once the fiftystater package is loaded, you can look at the data frame fifty_states to see what you’re working with. fiftystater’s fiftystates data: long lat order hole piece id group -85.07007 31.98070 1 FALSE 1 alabama Alabama.1 -85.11515 31.90742 2 FALSE 1 alabama Alabama.1 -85.13557 31.85488 3 FALSE 1 alabama Alabama.1 -85.13156 31.78381 4 FALSE 1 alabama Alabama.1 -85.13017 31.77885 5 FALSE 1 alabama Alabama.1 -85.11529 31.73157 6 FALSE 1 alabama Alabama.1 If we want to plot that map, all we have to do is call ggplot with geom_map on the dataset. You might notice that we also include expand_limits and coord_map. That’s just to make sure that the plot includes the whole picture, and that the gridlines are appropriate for a map. ggplot(fifty_states, aes(map_id=id)) + geom_map(map = fifty_states) + expand_limits(x = fifty_states$long, y = fifty_states$lat) + coord_map() Hmmm. We probably don’t want those axis labels, so let’s remove them. While we’re at it, let’s also remove the gridlines by changing the theme. ggplot(fifty_states, aes(map_id=id)) + geom_map(map = fifty_states) + expand_limits(x = fifty_states$long, y = fifty_states$lat) + coord_map() + scale_x_continuous(breaks = NULL) + scale_y_continuous(breaks = NULL) + labs(x = &quot;&quot;, y = &quot;&quot;) + theme(panel.background = element_blank()) Congratulations! You’ve made a map! 4.3 Plotting Data Over a Map Now we just need to plot the Laurel and Yanny data over the map. Each row of data (each tweet) already has a code for whether “Yanny” or “Laurel” won. To find out how this trended over regions, we can just plot within-region ratios of Laurel:Yanny wins. A ratio above 1 indicates a region trended toward Laurel, and a ratio below 1 indicates a region trended toward Yanny. First, let’s plot all U.S. larger regions. Multiple census regions make up a single larger region, so these regions span large parts of the U.S. map. We need to reformat our Twitter dataset into a summary dataset where each row represents not a single tweet, but rather a single state. Each row of that dataset (with each row representing a state) should have the ratio of Laurel to Yanny wins for that state’s datapoint. Let’s start with the largest region size, in the LargerRegion column. We’ll take the ratio of Laurel to Yanny choices by region, and then we’ll merge that back with the state variable. The fiftystater package plots by states, so we still need the data by state. choiceByReg &lt;- as.data.frame.matrix(table(dataset$LargerRegion, dataset$choice)) choiceByReg$neither &lt;- NULL choiceByReg$ratio &lt;- choiceByReg$laurel / choiceByReg$yanny choiceByReg$LargerRegion &lt;- row.names(choiceByReg) choiceByReg &lt;- right_join(choiceByReg, states, by=&quot;LargerRegion&quot;) choiceByReg &lt;- choiceByReg[!is.na(choiceByReg$ratio) &amp; choiceByReg$ratio != Inf &amp; choiceByReg$LargerRegion !=&quot;&quot;, ] That resulting dataset looks like this, with LargerRegion values duplicated in each state contained in that region (note rows 9 and 10, for example): laurel yanny ratio LargerRegion State Abbreviation CensusRegion 1 454 286 1.587413 South alabama AL East South Central 2 3 1 3.000000 Alaska alaska AK Alaska 3 311 180 1.727778 West arizona AZ Mountain 4 454 286 1.587413 South arkansas AR West South Central 5 311 180 1.727778 West california CA Pacific 6 311 180 1.727778 West colorado CO Mountain 7 225 129 1.744186 Northeast connecticut CT New England 8 225 129 1.744186 Northeast delaware DE New England 9 454 286 1.587413 South florida FL South Atlantic 10 454 286 1.587413 South georgia GA South Atlantic 13 311 180 1.727778 West idaho ID Mountain 14 269 125 2.152000 Midwest illinois IL East North Central 15 269 125 2.152000 Midwest indiana IN East North Central 16 269 125 2.152000 Midwest iowa IA West North Central 17 269 125 2.152000 Midwest kansas KS West North Central Now let’s plot by region. We use a similar call to ggplot that we did above for our simple map, but it uses that Laurel-Yanny state dataset, and we need to add an aesthetic to geom_map to fill by ratio (creating a heat map by ratio). The resulting color fill will have darker colors for a stronger Yanny trend and lighter colors for a strong Laurel trend. ggplot(choiceByReg, aes(map_id = State)) + geom_map(aes(fill=ratio), map = fifty_states) + expand_limits(x = fifty_states$long, y = fifty_states$lat) + coord_map() + scale_x_continuous(breaks = NULL) + scale_y_continuous(breaks = NULL) + labs(x = &quot;&quot;, y = &quot;&quot;) + theme(legend.position = &quot;bottom&quot;, panel.background = element_blank()) Ack! Why can’t we see any differentiation? Well, our table contains Guam and Puerto Rico, but fiftystater doesn’t inclde them so they don’t appear on the map. That’s too bad. Guam and Puerto Rico trend very Laurel, so that changes how the color scale shows up on the rest of the United States. Figuring out how to plot Guam and Puerto Rico on this map provides a good advanced exercise, but for now let’s see what the map looks like without these datapoints. choiceByReg &lt;- choiceByReg[! choiceByReg$State %in% c(&quot;gaum&quot;, &quot;puerto rico&quot;), ] ggplot(choiceByReg, aes(map_id = State)) + geom_map(aes(fill=ratio), map = fifty_states) + expand_limits(x = fifty_states$long, y = fifty_states$lat) + coord_map() + scale_x_continuous(breaks = NULL) + scale_y_continuous(breaks = NULL) + labs(x = &quot;&quot;, y = &quot;&quot;) + theme(legend.position = &quot;bottom&quot;, panel.background = element_blank()) It looks like, overall, there were no regions that overwhelmingly preferred Yanny. However, there were some regions with a very high Laurel preference, like the midwest, and some with a more moderate Laurel preference, like the west coast. Alaska seems very pro-Laurel. Just for fun, let’s play with the appearance of this map. We can add add our own colors with scale_fill_gradient(). ggplot(choiceByReg, aes(map_id = State)) + geom_map(aes(fill=ratio), map = fifty_states) + expand_limits(x = fifty_states$long, y = fifty_states$lat) + coord_map() + scale_x_continuous(breaks = NULL) + scale_y_continuous(breaks = NULL) + labs(x = &quot;&quot;, y = &quot;&quot;) + theme(legend.position = &quot;bottom&quot;, panel.background = element_blank()) + scale_fill_gradient(low =&quot;thistle4&quot;, high = &quot;thistle&quot;) That looks great. It at least seems like these regions differ in terms of their Laurel vs. Yanny preference. Do you feel suspicious, though? This map might look a little too clean, if you know what I mean. What if, inside those regions, a few states drive the whole trend? What if there’s disagreement within regions? Let’s dig deeper. Repeating the same steps as before but using State instead of LargerRegion as the tabling variable will yield the Laurel-Yanny ratio at a state level. Cutting the data off (somewhat arbitrarily) at n = 25 tweets will avoid including states with only a few tweets, which could potentially represent a bad sample. choiceByState &lt;- as.data.frame.matrix(table(dataset$State, dataset$choice)) ## Using State choiceByState$neither &lt;- NULL choiceByState &lt;- choiceByState[rowSums(choiceByState) &gt;= 25, ] choiceByState$ratio &lt;- choiceByState$laurel / choiceByState$yanny choiceByState$State &lt;- row.names(choiceByState) choiceByState &lt;- choiceByState[!is.na(choiceByState$ratio) &amp; choiceByState$ratio != Inf &amp; choiceByState$State !=&quot;&quot;, ] choiceByState &lt;- choiceByState[! choiceByState$State %in% c(&quot;Guam&quot;, &quot;Puerto Rico&quot;), ] ggplot(choiceByState, aes(map_id = State)) + geom_map(aes(fill=ratio), map = fifty_states) + expand_limits(x = fifty_states$long, y = fifty_states$lat) + coord_map() + scale_x_continuous(breaks = NULL) + scale_y_continuous(breaks = NULL) + labs(x = &quot;&quot;, y = &quot;&quot;) + theme(legend.position = &quot;bottom&quot;, panel.background = element_blank()) Oof. That forms a more complicated picture. Unstructured social media data can get messy! Many states show up as blank holes, because the dataset has insufficient data for those states. Plotting larger regions suggested that the south trended Yanny (dark blue), the midwest trended Laurel (light blue), and the west trended somewhere in the middle. Does this map support that? Missing data makes it hard to tell, but one spot that pops out is the Kentucky-Tenessee-North Carolina region. In the LargerRegion map, these areas formed a fairly Yanny area with the rest of the southeast. Here, they pop out on their and suggest a more strongly Laurel trend. What else can you spot? In this tutorial, we went through simple map plotting and explored the Laurel-Yanny debate as a case study. We plotted the data by larger U.S. region and by U.S. state. We never plotted by the medium-sized census regions, so that makes a good exercise for additional practice if that’s what you’re looking for. Keep at it and happy plotting! Please feel free to email me at aftoncoombs@gmail.com with any errata or questions. "]
]
